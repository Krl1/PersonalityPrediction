{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,SubsetRandomSampler, ConcatDataset\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from dataset import PersonalityDataset\n",
    "\n",
    "from models.mlp import MLPsimple\n",
    "from models.cnn8 import CNN8simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'BFD'\n",
    "dataset_type = 'enc'\n",
    "\n",
    "batch_type = 'original' if dataset_type=='enc' else 'normalized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,device,dataloader,loss_fn,optimizer,train_accuracy):\n",
    "    train_loss, train_correct, train_correct_ocean = 0.0, 0, 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch[batch_type], batch['label']\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32, device=output.device)\n",
    "        loss = loss_fn(output.flatten(), labels.flatten())\n",
    "        train_accuracy(output, labels.to(torch.int64))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        predictions = torch.where(output>0, 1, 0)\n",
    "        train_correct += (predictions == labels.to(torch.int64)).sum().item()\n",
    "        train_correct_ocean += (predictions == labels.to(torch.int64)).sum(dim=0)\n",
    "\n",
    "    return train_loss, train_correct, train_correct_ocean\n",
    "  \n",
    "def valid_epoch(model,device,dataloader,loss_fn,val_accuracy):\n",
    "    valid_loss, val_correct, val_correct_ocean = 0.0, 0, 0\n",
    "    model.eval()\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch[batch_type], batch['label']\n",
    "        images = images.to(device)\n",
    "        output = model(images)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32, device=output.device)\n",
    "        loss = loss_fn(output.flatten(),labels.flatten())\n",
    "        val_accuracy(output, labels.to(torch.int64))\n",
    "        valid_loss += loss.item() * images.size(0)\n",
    "        predictions = torch.where(output>0, 1, 0)\n",
    "        val_correct += (predictions == labels.to(torch.int64)).sum().item()\n",
    "        val_correct_ocean += (predictions == labels.to(torch.int64)).sum(dim=0)\n",
    "\n",
    "    return valid_loss, val_correct, val_correct_ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LocationConfig_data = f'data/{dataset_name}/{dataset_type}/'\n",
    "\n",
    "model_name = f'{dataset_name}_{dataset_type}'\n",
    "params = {}\n",
    "params['BFD_enc'] = {'batch_norm': True,'batch_size': 16,'dropout': 0.4,'lr': 0.001,'negative_slope': 0.05}\n",
    "params['BFD_gray'] = {'batch_norm': False,'batch_size': 16,'dropout': 0.4,'lr': 0.001,'negative_slope': 0.1}\n",
    "params['BFD_rgb'] = {'batch_norm': False,'batch_size': 8,'dropout': 0.0,'lr': 0.00005,'negative_slope': 0.02}\n",
    "params['ChaLearn_enc'] = {'batch_norm': False,'batch_size': 4,'dropout': 0.3,'lr': 0.001,'negative_slope': 0.1}\n",
    "params['ChaLearn_gray'] = {'batch_norm': False,'batch_size': 4,'dropout': 0.0,'lr': 0.001,'negative_slope': 0.01}\n",
    "params['ChaLearn_rgb'] = {'batch_norm': False,'batch_size': 8,'dropout': 0.0,'lr': 0.00005,'negative_slope': 0.1}\n",
    "\n",
    "epochs = {}\n",
    "epochs['BFD_enc'] = 10\n",
    "epochs['BFD_gray'] = 70\n",
    "epochs['BFD_rgb'] = 17\n",
    "epochs['ChaLearn_enc'] = 6\n",
    "epochs['ChaLearn_gray'] = 5\n",
    "epochs['ChaLearn_rgb'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: data/BFD/enc/train/train.pickle\n",
      "file: data/BFD/enc/test/test.pickle\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PersonalityDataset(Path(LocationConfig_data + 'train/'))\n",
    "test_dataset = PersonalityDataset(Path(LocationConfig_data + 'test/'))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "m=len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'BFD':\n",
    "    split_arange = np.arange(1, 41)\n",
    "else:\n",
    "    split_arange = list(pd.read_csv('data/ChaLearn/bigfive_labels_mean.csv', index_col=0)['ShortVideoName'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 396\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-93c60e5f3382>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32, device=output.device)\n",
      "<ipython-input-3-93c60e5f3382>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.float32, device=output.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 1 | E:1/5 Tra Loss:0.689 Test Loss:0.666 Tra Acc 58.08% | 58.08% Test Acc 68.64% | 68.64%\n",
      "[60.60606384277344, 64.39393615722656, 55.30303192138672, 54.797977447509766, 55.30303192138672]\n",
      "F 1 | E:2/5 Tra Loss:0.661 Test Loss:0.668 Tra Acc 60.86% | 59.47% Test Acc 70.00% | 69.32%\n",
      "[63.6363639831543, 66.66667175292969, 59.5959587097168, 60.85858154296875, 53.53535461425781]\n",
      "F 1 | E:3/5 Tra Loss:0.630 Test Loss:0.677 Tra Acc 66.11% | 61.68% Test Acc 54.55% | 64.39%\n",
      "[66.16161346435547, 69.69696807861328, 65.90909576416016, 72.97979736328125, 55.80807876586914]\n",
      "F 1 | E:4/5 Tra Loss:0.623 Test Loss:0.675 Tra Acc 66.67% | 62.93% Test Acc 55.00% | 62.05%\n",
      "[72.22222137451172, 70.45454406738281, 63.38383483886719, 68.18181610107422, 59.09090805053711]\n",
      "F 1 | E:5/5 Tra Loss:0.609 Test Loss:0.666 Tra Acc 68.74% | 64.09% Test Acc 57.73% | 61.18%\n",
      "[70.2020263671875, 71.96969604492188, 65.40403747558594, 73.48484802246094, 62.626258850097656]\n",
      "44 396\n",
      "Fold 2\n",
      "F 2 | E:1/5 Tra Loss:0.726 Test Loss:0.691 Tra Acc 51.92% | 51.92% Test Acc 58.18% | 58.18%\n",
      "[55.80807876586914, 42.171714782714844, 54.040401458740234, 52.77777862548828, 54.797977447509766]\n",
      "F 2 | E:2/5 Tra Loss:0.702 Test Loss:0.685 Tra Acc 55.00% | 53.46% Test Acc 55.91% | 57.05%\n",
      "[57.828285217285156, 44.4444465637207, 60.101009368896484, 56.06060791015625, 56.56565475463867]\n",
      "F 2 | E:3/5 Tra Loss:0.690 Test Loss:0.705 Tra Acc 54.90% | 53.94% Test Acc 49.55% | 54.55%\n",
      "[62.878787994384766, 42.929290771484375, 60.60606384277344, 52.77777862548828, 55.30303192138672]\n",
      "F 2 | E:4/5 Tra Loss:0.664 Test Loss:0.694 Tra Acc 58.99% | 55.20% Test Acc 53.18% | 54.20%\n",
      "[64.14141845703125, 46.212120056152344, 63.88888931274414, 60.35353469848633, 60.35353469848633]\n",
      "F 2 | E:5/5 Tra Loss:0.649 Test Loss:0.683 Tra Acc 62.78% | 56.72% Test Acc 58.64% | 55.09%\n",
      "[71.71717071533203, 49.49494934082031, 64.14141845703125, 64.6464614868164, 63.88888931274414]\n",
      "44 396\n",
      "Fold 3\n",
      "F 3 | E:1/5 Tra Loss:0.691 Test Loss:0.671 Tra Acc 57.22% | 57.22% Test Acc 79.09% | 79.09%\n",
      "[60.101009368896484, 50.75757598876953, 61.3636360168457, 57.070709228515625, 56.818180084228516]\n",
      "F 3 | E:2/5 Tra Loss:0.678 Test Loss:0.659 Tra Acc 58.59% | 57.90% Test Acc 79.09% | 79.09%\n",
      "[62.373741149902344, 51.262630462646484, 68.6868667602539, 54.292930603027344, 56.313133239746094]\n",
      "F 3 | E:3/5 Tra Loss:0.662 Test Loss:0.638 Tra Acc 61.26% | 59.02% Test Acc 78.64% | 78.94%\n",
      "[61.3636360168457, 58.33333206176758, 67.42424011230469, 60.35353469848633, 58.83838653564453]\n",
      "F 3 | E:4/5 Tra Loss:0.660 Test Loss:0.634 Tra Acc 61.67% | 59.68% Test Acc 78.18% | 78.75%\n",
      "[62.878787994384766, 61.3636360168457, 66.41413879394531, 59.5959587097168, 58.080810546875]\n",
      "F 3 | E:5/5 Tra Loss:0.641 Test Loss:0.631 Tra Acc 64.29% | 60.61% Test Acc 78.18% | 78.64%\n",
      "[64.89899444580078, 61.86868667602539, 75.0, 63.38383483886719, 56.313133239746094]\n",
      "44 396\n",
      "Fold 4\n",
      "F 4 | E:1/5 Tra Loss:0.732 Test Loss:0.672 Tra Acc 49.85% | 49.85% Test Acc 55.91% | 55.91%\n",
      "[61.86868667602539, 36.61616134643555, 61.61616516113281, 37.121212005615234, 52.02020263671875]\n",
      "F 4 | E:2/5 Tra Loss:0.700 Test Loss:0.668 Tra Acc 51.21% | 50.53% Test Acc 55.91% | 55.91%\n",
      "[63.38383483886719, 38.88888931274414, 63.88888931274414, 40.15151596069336, 49.747474670410156]\n",
      "F 4 | E:3/5 Tra Loss:0.688 Test Loss:0.650 Tra Acc 53.38% | 51.48% Test Acc 55.00% | 55.61%\n",
      "[70.2020263671875, 38.88888931274414, 64.6464614868164, 42.171714782714844, 51.010101318359375]\n",
      "F 4 | E:4/5 Tra Loss:0.670 Test Loss:0.639 Tra Acc 56.87% | 52.83% Test Acc 61.82% | 57.16%\n",
      "[72.7272720336914, 42.929290771484375, 65.1515121459961, 51.262630462646484, 52.27272415161133]\n",
      "F 4 | E:5/5 Tra Loss:0.659 Test Loss:0.637 Tra Acc 58.28% | 53.92% Test Acc 60.00% | 57.73%\n",
      "[71.46464538574219, 45.20201873779297, 68.18181610107422, 52.02020263671875, 54.54545593261719]\n",
      "44 396\n",
      "Fold 5\n",
      "F 5 | E:1/5 Tra Loss:0.731 Test Loss:0.679 Tra Acc 47.63% | 47.63% Test Acc 63.64% | 63.64%\n",
      "[49.747474670410156, 58.33333206176758, 28.535354614257812, 59.848487854003906, 41.666664123535156]\n",
      "F 5 | E:2/5 Tra Loss:0.692 Test Loss:0.669 Tra Acc 53.84% | 50.73% Test Acc 65.00% | 64.32%\n",
      "[60.85858154296875, 64.14141845703125, 32.070709228515625, 59.5959587097168, 52.52525329589844]\n",
      "F 5 | E:3/5 Tra Loss:0.671 Test Loss:0.644 Tra Acc 57.12% | 52.86% Test Acc 75.91% | 68.18%\n",
      "[59.34343338012695, 63.88888931274414, 38.38383865356445, 65.65657043457031, 58.33333206176758]\n",
      "F 5 | E:4/5 Tra Loss:0.667 Test Loss:0.626 Tra Acc 56.41% | 53.75% Test Acc 75.00% | 69.89%\n",
      "[61.3636360168457, 65.65657043457031, 39.39393997192383, 60.101009368896484, 55.55555725097656]\n",
      "F 5 | E:5/5 Tra Loss:0.650 Test Loss:0.626 Tra Acc 59.34% | 54.87% Test Acc 71.36% | 70.18%\n",
      "[68.4343490600586, 67.92929077148438, 41.666664123535156, 61.11111068725586, 57.57575607299805]\n",
      "44 396\n",
      "Fold 6\n",
      "F 6 | E:1/5 Tra Loss:0.748 Test Loss:0.734 Tra Acc 46.97% | 46.97% Test Acc 29.55% | 29.55%\n",
      "[46.7171745300293, 54.040401458740234, 33.838382720947266, 44.4444465637207, 55.80807876586914]\n",
      "F 6 | E:2/5 Tra Loss:0.715 Test Loss:0.705 Tra Acc 49.75% | 48.36% Test Acc 40.45% | 35.00%\n",
      "[50.50504684448242, 61.3636360168457, 34.5959587097168, 43.181819915771484, 59.09090805053711]\n",
      "F 6 | E:3/5 Tra Loss:0.694 Test Loss:0.669 Tra Acc 51.92% | 49.55% Test Acc 61.36% | 43.79%\n",
      "[50.0, 63.38383483886719, 40.65656661987305, 47.22222137451172, 58.33333206176758]\n",
      "F 6 | E:4/5 Tra Loss:0.677 Test Loss:0.655 Tra Acc 55.71% | 51.09% Test Acc 68.64% | 50.00%\n",
      "[54.292930603027344, 63.38383483886719, 42.171714782714844, 57.3232307434082, 61.3636360168457]\n",
      "F 6 | E:5/5 Tra Loss:0.657 Test Loss:0.633 Tra Acc 59.24% | 52.72% Test Acc 73.18% | 54.64%\n",
      "[57.57575607299805, 67.42424011230469, 44.94949722290039, 60.60606384277344, 65.65657043457031]\n",
      "44 396\n",
      "Fold 7\n",
      "F 7 | E:1/5 Tra Loss:0.744 Test Loss:0.706 Tra Acc 48.79% | 48.79% Test Acc 50.45% | 50.45%\n",
      "[44.191917419433594, 55.30303192138672, 45.9595947265625, 60.60606384277344, 37.878787994384766]\n",
      "F 7 | E:2/5 Tra Loss:0.717 Test Loss:0.701 Tra Acc 52.37% | 50.58% Test Acc 52.27% | 51.36%\n",
      "[47.97979736328125, 59.34343338012695, 53.787879943847656, 61.3636360168457, 39.39393997192383]\n",
      "F 7 | E:3/5 Tra Loss:0.692 Test Loss:0.687 Tra Acc 55.35% | 52.17% Test Acc 55.00% | 52.58%\n",
      "[51.51515197753906, 63.6363639831543, 58.33333206176758, 61.3636360168457, 41.919193267822266]\n",
      "F 7 | E:4/5 Tra Loss:0.674 Test Loss:0.672 Tra Acc 58.99% | 53.88% Test Acc 59.55% | 54.32%\n",
      "[55.80807876586914, 66.41413879394531, 62.626258850097656, 63.38383483886719, 46.7171745300293]\n",
      "F 7 | E:5/5 Tra Loss:0.649 Test Loss:0.660 Tra Acc 61.46% | 55.39% Test Acc 60.45% | 55.55%\n",
      "[60.101009368896484, 69.44444274902344, 64.6464614868164, 66.66667175292969, 46.46464538574219]\n",
      "44 396\n",
      "Fold 8\n",
      "F 8 | E:1/5 Tra Loss:0.748 Test Loss:0.729 Tra Acc 45.35% | 45.35% Test Acc 39.55% | 39.55%\n",
      "[48.73737335205078, 57.57575607299805, 29.7979793548584, 50.75757598876953, 39.89898681640625]\n",
      "F 8 | E:2/5 Tra Loss:0.704 Test Loss:0.727 Tra Acc 52.73% | 49.04% Test Acc 39.55% | 39.55%\n",
      "[60.85858154296875, 65.1515121459961, 30.050504684448242, 62.878787994384766, 44.69696807861328]\n",
      "F 8 | E:3/5 Tra Loss:0.686 Test Loss:0.723 Tra Acc 55.20% | 51.09% Test Acc 40.91% | 40.00%\n",
      "[68.93939208984375, 62.626258850097656, 32.57575607299805, 65.40403747558594, 46.46464538574219]\n",
      "F 8 | E:4/5 Tra Loss:0.670 Test Loss:0.733 Tra Acc 57.88% | 52.79% Test Acc 41.36% | 40.34%\n",
      "[71.96969604492188, 67.17171478271484, 38.6363639831543, 64.14141845703125, 47.47474670410156]\n",
      "F 8 | E:5/5 Tra Loss:0.658 Test Loss:0.740 Tra Acc 62.12% | 54.66% Test Acc 41.36% | 40.55%\n",
      "[76.01010131835938, 71.96969604492188, 44.4444465637207, 69.69696807861328, 48.48484802246094]\n",
      "44 396\n",
      "Fold 9\n",
      "F 9 | E:1/5 Tra Loss:0.725 Test Loss:0.709 Tra Acc 48.38% | 48.38% Test Acc 39.55% | 39.55%\n",
      "[50.50504684448242, 48.232322692871094, 48.232322692871094, 40.65656661987305, 54.292930603027344]\n",
      "F 9 | E:2/5 Tra Loss:0.694 Test Loss:0.713 Tra Acc 54.95% | 51.67% Test Acc 46.36% | 42.95%\n",
      "[64.89899444580078, 52.52525329589844, 54.040401458740234, 50.0, 53.2828254699707]\n",
      "F 9 | E:3/5 Tra Loss:0.680 Test Loss:0.722 Tra Acc 58.79% | 54.04% Test Acc 46.82% | 44.24%\n",
      "[62.878787994384766, 60.60606384277344, 56.06060791015625, 53.2828254699707, 61.11111068725586]\n",
      "F 9 | E:4/5 Tra Loss:0.651 Test Loss:0.735 Tra Acc 63.33% | 56.36% Test Acc 44.55% | 44.32%\n",
      "[67.17171478271484, 63.13131332397461, 62.626258850097656, 62.121212005615234, 61.61616516113281]\n",
      "F 9 | E:5/5 Tra Loss:0.639 Test Loss:0.735 Tra Acc 64.85% | 58.06% Test Acc 43.64% | 44.18%\n",
      "[66.919189453125, 61.3636360168457, 66.66667175292969, 68.4343490600586, 60.85858154296875]\n",
      "44 396\n",
      "Fold 10\n",
      "F 10 | E:1/5 Tra Loss:0.692 Test Loss:0.692 Tra Acc 55.30% | 55.30% Test Acc 43.64% | 43.64%\n",
      "[58.58585739135742, 47.97979736328125, 63.38383483886719, 53.03030014038086, 53.53535461425781]\n",
      "F 10 | E:2/5 Tra Loss:0.673 Test Loss:0.679 Tra Acc 57.78% | 56.54% Test Acc 55.00% | 49.32%\n",
      "[59.848487854003906, 48.232322692871094, 64.39393615722656, 56.818180084228516, 59.5959587097168]\n",
      "F 10 | E:3/5 Tra Loss:0.665 Test Loss:0.653 Tra Acc 57.88% | 56.99% Test Acc 68.64% | 55.76%\n",
      "[58.83838653564453, 54.040401458740234, 67.17171478271484, 56.313133239746094, 53.03030014038086]\n",
      "F 10 | E:4/5 Tra Loss:0.653 Test Loss:0.643 Tra Acc 60.45% | 57.85% Test Acc 70.45% | 59.43%\n",
      "[62.626258850097656, 54.292930603027344, 70.2020263671875, 59.34343338012695, 55.80807876586914]\n",
      "F 10 | E:5/5 Tra Loss:0.643 Test Loss:0.645 Tra Acc 63.74% | 59.03% Test Acc 70.91% | 61.73%\n",
      "[66.66667175292969, 56.818180084228516, 71.46464538574219, 64.6464614868164, 59.09090805053711]\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "\n",
    "for fold, (train_num,val_num) in enumerate(splits.split(split_arange)):\n",
    "    train_accuracy = Accuracy(threshold=0.0).cuda()\n",
    "    val_accuracy = Accuracy(threshold=0.0).cuda()\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    val_num_name = np.array(split_arange)[np.array(val_num)]\n",
    "    for i, data in enumerate(dataset):\n",
    "        if data['num'] in val_num_name:\n",
    "            val_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "    print(len(val_idx), len(train_idx))\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    criterion = nn.BCEWithLogitsLoss()  \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=params[model_name]['batch_size'], sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=params[model_name]['batch_size'], sampler=test_sampler)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if dataset_type=='enc':\n",
    "        model = MLPsimple(**params[model_name])\n",
    "    else:\n",
    "        model = CNN8simple(data_type=dataset_type, dataset=dataset_name, **params[model_name])\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[model_name]['lr'])\n",
    "\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[],'train_acc_2':[],'test_acc_2':[],'train_acc_ocean':[],'test_acc_ocean':[]}\n",
    "\n",
    "    for epoch in range(epochs[model_name]):\n",
    "        train_loss, train_correct, train_correct_ocean = train_epoch(model,device,train_loader,criterion,optimizer,train_accuracy)\n",
    "        test_loss, test_correct, test_correct_ocean = valid_epoch(model,device,test_loader,criterion,val_accuracy)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        train_acc = train_correct / (len(train_loader.sampler) * 5) * 100\n",
    "        train_acc_2 = train_accuracy.compute() * 100\n",
    "        train_acc_ocean = train_correct_ocean / len(train_loader.sampler) * 100\n",
    "        test_loss = test_loss / len(test_loader.sampler)\n",
    "        test_acc = test_correct / (len(test_loader.sampler) * 5) * 100\n",
    "        test_acc_2 = val_accuracy.compute() * 100\n",
    "        test_acc_ocean = test_correct_ocean / len(test_loader.sampler) * 100\n",
    "\n",
    "        print(\"F {} | E:{}/{} Tra Loss:{:.3f} Test Loss:{:.3f} Tra Acc {:.2f}% | {:.2f}% Test Acc {:.2f}% | {:.2f}%\".format(\n",
    "            fold + 1,\n",
    "            epoch + 1,\n",
    "            epochs[model_name],\n",
    "            train_loss,\n",
    "            test_loss,\n",
    "            train_acc,\n",
    "            train_acc_2,\n",
    "            test_acc,\n",
    "            test_acc_2\n",
    "            ))\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['train_acc_2'].append(train_acc_2.item())\n",
    "        history['test_acc_2'].append(test_acc_2.item())\n",
    "        print([t.item() for t in train_acc_ocean])\n",
    "        history['train_acc_ocean'].append([t.item() for t in train_acc_ocean])\n",
    "        history['test_acc_ocean'].append([t.item() for t in test_acc_ocean])\n",
    "\n",
    "    foldperf['fold{}'.format(fold+1)] = history  \n",
    "\n",
    "torch.save(model,f'model/k_cross/{dataset_name}_{dataset_type}.pt')\n",
    "a_file = open(f'results/{dataset_name}_{dataset_type}.pkl', 'wb')\n",
    "pickle.dump(foldperf, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ur",
   "language": "python",
   "name": "ur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
